/*************
* Moses Nakamura
* mnn2104
**************/

Q. 4
The model is not very good.
The results were:
Found 16130 NEs. Expected 5931 NEs; Correct: 1354.
	 precision 	recall 		F1-Score
Total:	 0.083943	0.228292	0.122751
PER:	 0.063737	0.166485	0.092183
ORG:	 0.100000	0.245889	0.142178
LOC:	 0.187053	0.285169	0.225918
MISC:	 0.037383	0.212812	0.063595

We can find this by running emitter.py from within the mnn2104_h1 directory.  It will populate the prediction.file file.  This file depends upon the ner.counts_rare file, which must be populated by running rarify.py and then piping the count_freqs.py output for ner_train_rare.dat to ner.counts_rare.

Frankly, across the board, it's awful.  Worse than that though, I also ran the same test without modifying for "_RARE_" and got more correct:

Found 17011 NEs. Expected 5931 NEs; Correct: 2108.

	 precision 	recall 		F1-Score
Total:	 0.123920	0.355421	0.183768
PER:	 0.126736	0.352557	0.186448
ORG:	 0.147181	0.396114	0.214618
LOC:	 0.229585	0.374046	0.284529
MISC:	 0.045960	0.264929	0.078331

which seems to suggest that making things rare is not useful yet.
We can find this by running emitter_no_rare.py from within the mnn2104_h1 directory.  It will populate the prediction_no_rare.file file.

The threegram.py fulfils the requirement of being able to classify based on a count file, and also to print.  Sample program:
tagger = Tagger()
tagger.classify("count.file")
tagger.simple_printer("out.file")

Rarifying and also using viterbi's however, gives great improvement.  The result we found was:

Found 4669 NEs. Expected 5931 NEs; Correct: 3656.

	 precision 	recall 		F1-Score
Total:	 0.783037	0.616422	0.689811
PER:	 0.732850	0.604461	0.662493
ORG:	 0.659067	0.475336	0.552323
LOC:	 0.897111	0.694111	0.782662
MISC:	 0.827048	0.690554	0.752663

so we have become fairly precise.
This again used ner.counts_rare file, and can be run by running viterbi.py, which populates prediction.file

For the rare buckets that were hand chosen, I looked at the things that were popping up for rare, and found that there were a few common buckets.  The biggest ones were proper nouns that started with a capital letter, numbers, words that contained numbers, hyphenated words, words in all-caps.  After removing those, the only words remaining were genuinely unusual, but standard English words.
Found 5723 NEs. Expected 5931 NEs; Correct: 4276.

	 precision 	recall 		F1-Score
Total:	 0.747161	0.720958	0.733825
PER:	 0.815446	0.781284	0.797999
ORG:	 0.527794	0.674141	0.592058
LOC:	 0.870557	0.707743	0.780752
MISC:	 0.839895	0.694897	0.760547
This depends upon ner_train_mine.dat, which is generated by running minify.py then is taken as a command line argument by count_freqs.py and piped into ner.counts_mine.  Once this has been done, run viterbi_mine.py to populate my_prediction.file.  Note: only count_freqs takes command line arguments--the rest of the files have hardcoded the file names.
